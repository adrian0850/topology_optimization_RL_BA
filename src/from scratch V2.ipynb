{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "in this cell the used variables can be changed \n",
    "\n",
    "height and width correspond to the dimensions of the Workspace, in which the Topology Optimisation is going to be performed\n",
    "\n",
    "log_dir is the direction where the current best Model will be saved to during training\n",
    "\n",
    "ts is the amount of timesteps the model will be trained for\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "import autograd.numpy as anp\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import results_plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalsX\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "normalsY\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "height = 13\n",
    "width = 19\n",
    "log_dir = \"log\"\n",
    "ts = 1e6\n",
    "\n",
    "density = 1e-4\n",
    "constraint_thresh = 0.7\n",
    "\n",
    "\n",
    "normals = np.zeros((height, width, 2))\n",
    "x = 0\n",
    "y = 1\n",
    "print(\"normalsX\\n\" + str(normals[:,:,x]))\n",
    "print(\"normalsY\\n\" + str(normals[:,:,y]))\n",
    "\n",
    "\n",
    "\n",
    "number_of_nodes = height * width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the reward for each state of the workspace is the compliance\n",
    "which should be minimized or above a certain threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO write the function to get the reward\n",
    "# It should return the compliance of the structure\n",
    "# The compliance is the sum of the forces applied to the structure\n",
    "# multiplied by the displacement of the nodes\n",
    "# The goal of the Optimization will be to minimize the compliance or keep it below a certain threshold\n",
    "\n",
    "\n",
    "def rewardFunction(state):\n",
    "    return 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def constraintFunction(state):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopOptEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "        self.design_space = anp.ones((height, width))\n",
    "        self.bound_nodes = np.zeros((height, width))\n",
    "        self.force_nodes = np.zeros((height, width))\n",
    "        self.density_matrix = self.design_space * density\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        self.action_space = gym.spaces.Discrete(number_of_nodes)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(height, width), dtype=np.float64)\n",
    "\n",
    "        # A Dictionary is used to map each coordinate tuple of the designspace to\n",
    "        # a singular distinct integer for use in the \n",
    "        k=0\n",
    "        for i in range(self.n):\n",
    "            for j in range(self.m):\n",
    "                self._actions_to_coordinates[k] = (i,j)\n",
    "                k += 1\n",
    "\n",
    "\n",
    "        self.n, self.m = self.design_space.shape\n",
    "        print(\"self.n:\\t\" + str(self.n) + \"\\nself.m:\\t\" + str(self.m))\n",
    "        self._actions_to_coordinates = {} \n",
    "        self.reward = 0\n",
    "        self.step_count = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        #\n",
    "        # TODO finisch the step function for the environment\t\n",
    "        # we need to add the reward function and the termination condition\n",
    "        #\n",
    "        #\n",
    "        # action is the coordinate of the node that is going to be changed\n",
    "        # action is a number between 0 and number_of_nodes\n",
    "\n",
    "        self.step_count += 1\n",
    "        terminated = False\n",
    "\n",
    "        # exectute an action and get the Reward\n",
    "        x,y = self._actions_to_coordinates[action]\n",
    "        self.design_space[x][y] = 0\n",
    "        self.reward = rewardFunction(self.design_space)\n",
    "        self.constraint = constraintFunction(self.design_space)\n",
    "\n",
    "        # \n",
    "        if self.constraint > constraint_thresh:\n",
    "            self.reward -= 1\n",
    "\n",
    "        \n",
    "        if self.step_count > number_of_nodes:\n",
    "            terminated = True\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        return self.design_space, self.reward, terminated, False, self._getInfo()\n",
    "    \n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        # Reset the environment\n",
    "        # This function returns the initial state of the environment\n",
    "        # The initial state should be a 2D array of size (height, width)\n",
    "        #\n",
    "        # TODO: add a way to introduce Randomness to the Forces applied in the Design space\n",
    "        #\n",
    "\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        self.step_count = 0\n",
    "        self.design_space = anp.ones((height, width))\n",
    "        self.density_matrix = self.design_space * density \n",
    "\n",
    "\n",
    "        return self.design_space, self._getInfo()\n",
    "    \n",
    "\n",
    "    \n",
    "    def _getInfo(self):\n",
    "        # This function returns the information about the environment\n",
    "        # This function is used to monitor the environment\n",
    "        # The information should be a dictionary\n",
    "        # The dictionary should contain the following keys:\n",
    "        # - step_count: the number of steps that have been executed\n",
    "        # - current Reward: the reward of the current state\n",
    "        # - design_space: the current state of the environment\n",
    "        return {\"step_count\": self.step_count, \n",
    "                \"current Reward\": self.reward,\n",
    "                \"design_space\": self.design_space}\n",
    "    \n",
    "    def debug(self):\n",
    "        print(self.design_space)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.n:\t13\n",
      "self.m:\t19\n",
      "[[1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "env = TopOptEnv()\n",
    "check_env(env, warn=True)\n",
    "env.debug()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
